# Проект на тему: "Алгоритм обучения с подкреплением "
Выполнили студентки 3-го курса, группы НФИбд-02-19: Кондрашина Мария (1032192863) и Комарова Ирина (1032192866)
# Ход выполнения проекта

Часть 1: Познакомиться с алгоритмом обучения с подкреплением DQN. Установите библиотеку PyTorch и разберитесь с кодом https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html . 

Часть 2: познакомиться с применением для Atari https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf ; 

Часть 3: Попробуйте реализовать алгоритм без участия TF, Torch, тд.

# Познакомились с алгоритмом обучения с подкреплением DQN
DQN - глубокая Q сеть. Другими словами - использование глубоких нейронных сетей для Q-обучения.
Для обучения Q-функции (функции ценности действия) используется уравнение Беллмана, согласно которому значения Q-функции обновляются с учетом ценностей последующих пар состояние–действие. 

Это дает алгоритму возможность обучаться на каждом шаге, не дожидаясь завершения выполнения(например победы в игре). Кроме того, ценности состояний или пар состояние–действие хранятся в таблице - Q-таблице.

Алгоритм DQN состоит из трех основных частей: 
1) сбор и сохранение данных. 
2) оптимизация нейронной сети 
3) обновление целевой сети

# DQN-learning на практике
1. Установили библиотеку PyTorch;
2. Изучили код https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html

![lv_0_20211019155528](https://user-images.githubusercontent.com/92932414/138299375-85315bff-04c3-4953-bcbe-e26102b7573b.gif)
![lv_0_20211019160527](https://user-images.githubusercontent.com/92932414/138299400-3e4e182a-1895-4a36-9338-66f56eb2e1ea.gif)


# Познакомились с применением для Atari https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf
В данной статье представлена модель глубокого обучения для обучения с подкреплением и продемонстрирована ее эффективность в освоении процесса игры в компьютерные игры Atari 2600, используя только необработанные пиксели как входные данные

Что такое Atari? - игры на игровой приставке. Причем, игры Atari далеко не тривиальны, и во многих необходимо сложное планирование. 

Особенность этих игр состоит в частичной наблюдаемости, поскольку агент видит только изображения. Полное состояние окружающей среды остается неизвестным.

Например, алгоритм глубокого Q-обучения применим для обучения играм Atari, располагая лишь изображениями, вознаграждением и сигналом завершения. 

# Пример игр Atari
![breakout](https://user-images.githubusercontent.com/92932414/138299156-32d8478b-0905-40f8-a904-5c0fda82da1d.gif)
![qbert](https://user-images.githubusercontent.com/92932414/138299246-f7cf4f8d-f390-497e-a99b-b758e55df094.gif)

# Реализовали алгоритм без участия TF, Torch, тд. Результат выполнения нашего кода

![lv_0_20211019202759](https://user-images.githubusercontent.com/92932414/138299992-0dea7658-6047-4f6c-8ab8-5649a89c20dd.gif)
![lv_0_20211019230436](https://user-images.githubusercontent.com/92932414/138299913-b8c1846a-7d86-4de9-b4be-896f0e65f8d4.gif)
![image](https://user-images.githubusercontent.com/92932414/138300709-6d564f23-9620-4b54-8a23-1bc8777b6bfe.png)

Также ссылка на дополнительный источник: https://www.rulit.me/data/programs/resources/pdf/Algoritmy-obucheniya-s-podkrepleniem-na-Python_RuLit_Me_650280.pdf
